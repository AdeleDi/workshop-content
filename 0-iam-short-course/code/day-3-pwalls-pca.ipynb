{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bcdata Workshop 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic machine learning with `scikit-learn`\n",
    "\n",
    "* Machine learning terminology\n",
    "* Example: MNIST digits\n",
    "* PCA with `sklearn`\n",
    "* Build our own PCA function\n",
    "* $k$-means clustering using `sklearn`\n",
    "* $k$NN classification using `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning terminology\n",
    "\n",
    "* Sample: element in the dataset represented by a numeric vector (a measurement)\n",
    "* Feature: a value in the vector representing a sample (a co-variate)\n",
    "* Target: the output label attached to a sample (a label, response)\n",
    "* Supervised learning: training data is labelled\n",
    "    * classification: output is discrete\n",
    "    * regression: output is continuous\n",
    "* Unsupervised learning: training data is not labelled\n",
    "    * Clustering\n",
    "* Train and test data: trianing data is what you build your model with and the test data is what you use to evaluate your model (it's common to split the original into a training set and a separate testing set). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Hand-written digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(digits.images[0,...], cmap='binary', interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal component analysis\n",
    "\n",
    "The idea is: suppose that we have a dataset stored in a matrix $X$ where each row is a sample and normalized (such that the mean value in each column is 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I'm going to choose `n_components = 29` because that is the lowest number that explains $\\geq 95 \\%$ of the variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=29)\n",
    "pca.fit(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(pca.components_[0,:].reshape((8,8)), cmap='binary', interpolation='gaussian')\n",
    "plt.subplot(122)\n",
    "plt.imshow(pca.components_[1,:].reshape((8,8)), cmap='binary', interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are they orthogonal to each other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('<PCA1, PCA2> = {}'.format(np.dot(pca.components_[0,:], pca.components_[1,:]).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2d = pca.transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_colours = matplotlib.cm.get_cmap('viridis', 10)\n",
    "plt.figure(figsize=(12,6));\n",
    "plt.scatter(data2d[:,0], data2d[:,1], c=digits.target, cmap=discrete_colours, vmin=-0.5, vmax=9.5);\n",
    "plt.colorbar(ticks=np.arange(10));\n",
    "plt.xlabel('Looks like 3 not 4')\n",
    "plt.ylabel('Looks like 0 not 1');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    plt.imshow(pca.components_[i,:].reshape((8,8)), cmap='binary', interpolation='gaussian')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling our own PCA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ourPCA(data, n_components=2):\n",
    "    \n",
    "    # normalize the input data (mean-value of 0 down the columns)\n",
    "    means = data.mean(axis=0)\n",
    "    X = data - means\n",
    "    evals, evecs = eig(X.T @ X)\n",
    "    indices = np.argsort(evals.real)[::-1][:n_components]\n",
    "    return evecs[:, indices].T.real\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc5 = ourPCA(digits.data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "for j in range(5):\n",
    "    plt.subplot(1,5,j+1)\n",
    "    plt.imshow(pc5[j,:].reshape((8,8)), cmap='gray', interpolation='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Let $v, w$ be eigenvectors of $X^T X$ corresponding to eigenvalues $\\lambda_1 \\geq \\lambda_2$ respectively.\n",
    "In PCA, $v$ is the *first principal component*. To find the second pc, one proceeds by projecting $X$ onto $v^\\perp$ to obtain $Y := \\mathcal{P}_{v^\\perp} (X)$ and then computes the leading eigenvector of $Y^T Y$. Prove that this leading eigenvector is $w$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = digits.data[digits.target == 1]\n",
    "ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ones[20,:].reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ones.mean(axis=0).reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onesProj = pca.transform(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(onesProj[:,0], onesProj[:,1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index1 = np.argmax(onesProj[:,0])\n",
    "index2 = np.argmax(onesProj[:,1])\n",
    "outlier1 = ones[index1, :].reshape((8,8))\n",
    "outlier2 = ones[index2, :].reshape((8,8))\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.subplot(121)\n",
    "plt.imshow(outlier1)\n",
    "plt.subplot(122)\n",
    "plt.imshow(outlier2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4\n",
    "clf = KMeans(n_clusters=N, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(np.minimum(20, 4*N),4))\n",
    "\n",
    "for i in range(N):\n",
    "    plt.subplot(1,N,i+1)\n",
    "    plt.imshow(clf.cluster_centers_[i,:].reshape((8,8)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $k$-nearest neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in (x_train, x_test, y_train, y_test):\n",
    "    print(var.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, adjusted_mutual_info_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_score = accuracy_score(y_test, y_pred)\n",
    "I_mutual = adjusted_mutual_info_score(y_test, y_pred)\n",
    "print('Accuracy score = {};\\nAdjusted Mutual Information score = {}'.format(acc_score.round(3), I_mutual.round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Classification report')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Confusion matrix')\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
